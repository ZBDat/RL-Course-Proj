============= FINAL RESULT ============
Q-learning

Iterations: 100000
Maximum Q values:
 5.2002 | 6.0195 | 7.1922 | 8.6804 |
 4.5327 |   #    | 3.2047 | -96.9968 |
 3.9333 | 2.4502 | 2.4759 | 1.1601 |
Final policy:
 4 | 4 | 4 | 1 |
 1 | # | 3 | 3 |
 1 | 4 | 1 | 4 |


SARSA

Iterations: 1000000
Maximum of action values
 1.28 | 1.4907 | 1.8059 | 3.3445 |
 1.1184 |   #  | -0.9753 | -101.1819 |
 0.9745 | 0.8494 | 0.507 | -2.6574 |
Values for action 'UP'
 1.1564 | 1.3274 | 1.5462 | 3.3445 |
 1.1184 |   #  | -8.808 | -107.288 |
 0.9745 | 0.7126 | -1.5745 | -72.5499 |
Values for action 'DOWN'
 1.0293 | 1.312 | -0.9666 | -66.4311 |
 0.8861 |   #  | -6.1946 | -110.5693 |
 0.8289 | 0.6932 | -0.2129 | -12.6515 |
Values for action 'LEFT'
 1.1196 | 1.1743 | 1.0358 | -9.064 |
 0.9766 |   #  | -0.9753 | -101.1819 |
 0.8408 | 0.8494 | 0.507 | -9.0984 |
Values for action 'RIGHT'
 1.28 | 1.4907 | 1.8059 | -5.5647 |
 0.97 |   #  | -71.2874 | -174.698 |
 0.7507 | 0.4709 | -2.2513 | -2.6574 |
Final policy:
 4 | 4 | 4 | 1 |
 1 | # | 3 | 3 |
 1 | 3 | 3 | 4 |
